---
title: "Coursera Data Science Capstone: Milestone Report"
author: "Boris Romanciuc"
date: December 28, 2018
output: html_document
---

##Introduction
This R Markdown presentation is the Milestone Report for the Coursera Data Science Capstone final project. The goal of the capstone project is to explore the most important features of a text data set offered by Johns Hopkins University within this capstone project of the Data Science Specialization in partnership with Swiftkey.

As a result of this exploration, we have to create a predictive text model using a large text corpus of different type of documents as training data, like blogs, news, twitter. We will perform an exploratory data analysis over this corpus of text data and will use common natural language processing techniques to build a predictive model by the final of this capstone project.

This predictive model is to be implemented in a final application using the R's Shiny package that will predict the next word to be typed using a natural language processing model.
We will have to make certain transformations over the text data corpus before building the predictive model, i.e. removing unnecessary noises, such as certain kinds of characters and words (stopwords, offensive words), html links, tags, punctuations, symbol and digits in order to aid in prediction accuracy of the model.
As a result with all these transformation we will end up with several list of tokens - some lists containing single words and also two, three and four word phrases that occur most frequently in our test data set of text corpus. 

This data set consist of text in different languages like Russian, German, Finnish and English. We will be using the English set and will load the files inside the "en_US" folder which contains three text files sourced from blogs, news and twitter feeds: "en_US.blogs.txt", "en_US.news.txt" and "en_US.twitter.txt".

## Downloading the Data

We will download the zip file that contains the text files from this link <https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip>.

Next step, we will read the text files into R using the `readLines()` function.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE)
```

```{r get_data}
# Download the data zip file and unzip it to the local disk
if (!file.exists("Coursera-SwiftKey.zip")) {
  download.file("https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip")
  unzip("Coursera-SwiftKey.zip")
}

#Read the data
blogs.data <- readLines(con = "./final/en_US/en_US.blogs.txt", encoding= "UTF-8", skipNul = TRUE)
news.data <- readLines(con = "./final/en_US/en_US.news.txt", encoding= "UTF-8", skipNul = TRUE)
twitter.data <- readLines(con = "./final/en_US/en_US.twitter.txt", encoding= "UTF-8", skipNul = TRUE)
```

In this step we will have a look over the data sets and make a summary of it as follows (file sizes, line counts, word counts and mean words per line):

```{r summary}
#Loading the necesary library
library(stringi)

# Get the size of files
blogs.data.size <- file.info("final/en_US/en_US.blogs.txt")$size / 1024 ^ 2
news.data.size <- file.info("final/en_US/en_US.news.txt")$size / 1024 ^ 2
twitter.data.size <- file.info("final/en_US/en_US.twitter.txt")$size / 1024 ^ 2

# Get the number of words in files
blogs.data.words <- stri_count_words(blogs.data)
news.data.words <- stri_count_words(news.data)
twitter.data.words <- stri_count_words(twitter.data)

# Summary of the data sets
data.frame(source = c("blogs file", "news file", "twitter file"),
           file_size_MB = c(blogs.data.size, news.data.size, twitter.data.size),
           num_lines = c(length(blogs.data), length(news.data), length(twitter.data)),
           num_words = c(sum(blogs.data.words), sum(news.data.words), sum(twitter.data.words)),
           mean_num_words = c(mean(blogs.data.words), mean(news.data.words), mean(twitter.data.words)))
```


## Cleaning The Data

In this step we will perform data cleaning and some transformation over the data set. This means that we will remove the so called noise like punctuations, numbers, excess whitespace, unicode tags, twitter hashtags, URL links, special characters, stopwords, profanity words from this [list](https://www.freewebheaders.com/download/files/full-list-of-bad-words_text-file_2018_07_30.zip). 
As for transformations we will change the text to lower case, check and transform the character's type to ASCII to eliminate foreign letters and unnecesarry symbols.
As the data set is very large and it will require also large computational resources and times, we will randomly select just a 1% from each data set. This would be enough for the demonstrating purpose of data preprocessing and exploratory data analysis.
However, when we will create the prediction algorithm for our Shiny app, we will be using the full dataset.

```{r corpus}
# Loading the necesary library
library(tm)

# Taking a sample of the data
set.seed(555)
data.sample <- c(sample(blogs.data, length(blogs.data) * 0.01),
                 sample(news.data, length(news.data) * 0.01),
                 sample(twitter.data, length(twitter.data) * 0.01))

# Create corpus and clean the data
corp <- VCorpus(VectorSource(data.sample))

#Transform the character's type to ASCII to eliminate foreign letters and unnecesarry symbols and rewrite the corpus
temps <- sapply(corp, function(row) iconv(row, "latin1", "ASCII", sub=""))
corp <- VCorpus(VectorSource(temps)); rm(temps)

```


### Checking different types of characters
```{r chartype}
#Loading the necesary library
library(stringr)
library(dplyr)

#The function for checking the character's type
chartype <- function(crp, nr=seq(crp)) {
    # crp: the corpus to be parsed
    # nr: the elements of the corpus for which characters will be returned
    ch <- character()
    for(i in nr){
        ch <- c(ch, crp[[i]][[1]])
    }
    ch %>%
    str_split("") %>%
    sapply(function(crp) crp[-1]) %>%
    unlist %>%
    unique %>%
    sort(dec=F)
}
chars <- chartype(corp)
print(chars, quote = F)
```
As we can see, there are different symbols and numbers that are not serving for prediction purposes. So we will remove them along with the rest of unnecessary chunk words and other types of so called noise. 
```{r data_clean}
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
corp <- tm_map(corp, toSpace, "(f|ht)tp(s?)://(.*)[.][a-z]+")
corp <- tm_map(corp, toSpace, "@[^\\s]+")
corp <- tm_map(corp, removePunctuation)
corp <- tm_map(corp, stripWhitespace)
corp <- tm_map(corp, removeWords, stopwords("en"))
corp <- tm_map(corp, tolower)
corp <- tm_map(corp, removeNumbers)
corp <- tm_map(corp, PlainTextDocument)
bad_words <- readLines(con="./full-list-of-bad-words_text-file_2018_07_30.txt", skipNul = TRUE)
corp <- tm_map(corp, removeWords, bad_words) 

#Check again the type of remained characters
chars <- chartype(corp)
print(chars, quote = F)
```

## Exploratory Analysis
Now, our data set (corpus) is much more cleaner as we can see. After these transformations, we can now proceed to tokenization, i.e. doing some exploratory analysis to determine the most frequent unigrams, bigrams, trigrams and quadgrams using the `RWeka` package and the `TermDocumentMatrix()` function from `tm` package.

```{r ngrams}
#Loading the necesary libraries
library(RWeka)
library(ggplot2)

options(mc.cores=1)

getFreq <- function(tdm) {
  freq <- sort(rowSums(as.matrix(tdm)), decreasing = TRUE)
  return(data.frame(word = names(freq), freq = freq))
}
bigram <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
trigram <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
quadgram <- function(x) NGramTokenizer(x, Weka_control(min = 4, max = 4))

#Plot function for first 20 elements
ploting <- function(data, label) {
  ggplot(data[1:20,], aes(x=reorder(word,freq), y=freq, fill=freq)) +
    geom_bar(stat="identity") +
    theme_bw() +
    coord_flip() +
    theme(axis.title.y = element_blank()) +
    labs(y="Frequency", title=label)
}

# Get frequencies of most common n-grams in data sample
f_unigram <- getFreq(removeSparseTerms(TermDocumentMatrix(corp), 0.9999))
f_bigram <- getFreq(removeSparseTerms(TermDocumentMatrix(corp, control = list(tokenize = bigram)), 0.9999))
f_trigram <- getFreq(removeSparseTerms(TermDocumentMatrix(corp, control = list(tokenize = trigram)), 0.9999))
f_quadgram <- getFreq(removeSparseTerms(TermDocumentMatrix(corp, control = list(tokenize = quadgram)), 0.9999))
```

Ploting the histogram of the 20 most common unigrams in the data sample.
```{r unigrams}
ploting(f_bigram, "20 Most Common Unigrams")
```

Ploting the histogram of the 20 most common bigrams in the data sample.
```{r bigrams}
ploting(f_bigram, "20 Most Common Bigrams")
```

Ploting the histogram of the 20 most common trigrams in the data sample.
```{r trigrams}
ploting(f_trigram, "20 Most Common Trigrams")
```

Ploting the histogram of the 20 most common quadgrams in the data sample.
```{r quadgrams}
ploting(f_quadgram, "20 Most Common Quadgrams")
```


## Next steps for prediction algorithm and plans for Shiny app

As a result of our exploratory analysis we came up with several frequency lists of n-gram tokens. But for the final algorithm we will use the whole data set of text files to extract these frequency lists. These lists will be used to build a frequency look-up table of the n-gram model that will be used to build our predictive algorithm for our Shiny app.

A possible strategy of prediction will be using the 4-gram model to predict the next word to be typed. If no result found, then the next n-gram model is to be used, i.e. the 3-gram model. If again no match will be found, then the 2-gram model is to be used.

In our Shiny application we will have a text input box that the user will use to enter some phrase, so that the app will provide reactive predictions by suggesting the most likely next word to continue the already typed phrase. As a plan we will want to offer the user a list of suggested words to update the next word to be typed.